{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch_geometric.datasets import DBLP\n",
    "from torch_geometric.utils import to_torch_sparse_tensor\n",
    "\n",
    "import torch_geometric.transforms as T\n",
    "import torch.nn.functional as F\n",
    "import graph_polluters\n",
    "import matplotlib.pyplot as plt\n",
    "from copy import deepcopy\n",
    "\n",
    "path = './data/dblp'\n",
    "# We initialize conference node features with a single one-vector as feature:\n",
    "dataset = DBLP(path, transform=T.Constant(node_types='conference'))\n",
    "data = dataset[0]\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sparsify the graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = graph_polluters.remove_edges(data, 0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get conferences where authors have had papers in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "author_to_paper = to_torch_sparse_tensor(data['author', 'to', 'paper'].edge_index, size=(4057,14328)).detach().clone()\n",
    "\n",
    "paper_to_conference = to_torch_sparse_tensor(data['paper', 'to', 'conference'].edge_index, size=(14328,20)).detach().clone()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_211455/1147041153.py:1: UserWarning: Sparse CSR tensor support is in beta state. If you miss a functionality in the sparse tensor support, please submit a feature request to https://github.com/pytorch/pytorch/issues. (Triggered internally at ../aten/src/ATen/SparseCsrTensorImpl.cpp:53.)\n",
      "  author_to_conference = author_to_paper @ paper_to_conference\n"
     ]
    }
   ],
   "source": [
    "author_to_conference = author_to_paper @ paper_to_conference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 4.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0.,  ..., 1., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 1., 0., 0.]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.mm(author_to_paper, paper_to_conference).to_dense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(indices=tensor([[   1,    1,    2,  ..., 4052, 4054, 4056],\n",
       "                       [   2,    9,   15,  ...,   17,   17,   17]]),\n",
       "       values=tensor([4., 1., 1.,  ..., 1., 1., 1.]),\n",
       "       size=(4057, 20), nnz=3524, layout=torch.sparse_coo)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "author_to_conference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.as_tensor([1,2,3])\n",
    "b = ['a', 'b', 'c']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1],\n",
       "        [2],\n",
       "        [3]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.unsqueeze(-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use cosine similarity to tell wheter to add new edge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_similar_edges(adjacency_matrix, min_similarity):\n",
    "    adjacency_matrix = adjacency_matrix.to(device)\n",
    "\n",
    "    indices = torch.combinations(torch.arange(adjacency_matrix.shape[0])).to(device)\n",
    "    first_embeddings = adjacency_matrix[indices[:,0]]\n",
    "    second_embeddings = adjacency_matrix[indices[:,1]]\n",
    "    similarities = torch.nn.functional.cosine_similarity(first_embeddings, second_embeddings, dim=1)\n",
    "    return indices[similarities > min_similarity].T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "NotImplementedError",
     "evalue": "Could not run 'aten::index.Tensor' with arguments from the 'SparseCUDA' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::index.Tensor' is only available for these backends: [CPU, CUDA, HIP, MPS, IPU, XPU, HPU, VE, MTIA, PrivateUse1, PrivateUse2, PrivateUse3, Meta, FPGA, ORT, Vulkan, Metal, QuantizedCPU, QuantizedCUDA, QuantizedHIP, QuantizedMPS, QuantizedIPU, QuantizedXPU, QuantizedHPU, QuantizedVE, QuantizedMTIA, QuantizedPrivateUse1, QuantizedPrivateUse2, QuantizedPrivateUse3, QuantizedMeta, CustomRNGKeyId, MkldnnCPU, SparseCsrCPU, SparseCsrCUDA, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMTIA, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradMeta, AutogradNestedTensor, Tracer, AutocastCPU, AutocastCUDA, FuncTorchBatched, BatchedNestedTensor, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PreDispatch, PythonDispatcher].\n\nUndefined: registered at aten/src/ATen/RegisterCompositeExplicitAutogradNonFunctional.cpp:21592 [default backend kernel]\nCPU: registered at aten/src/ATen/RegisterCPU.cpp:31357 [kernel]\nCUDA: registered at aten/src/ATen/RegisterCUDA.cpp:44411 [kernel]\nHIP: registered at aten/src/ATen/RegisterCompositeExplicitAutogradNonFunctional.cpp:21592 [default backend kernel]\nMPS: registered at aten/src/ATen/RegisterCompositeExplicitAutogradNonFunctional.cpp:21592 [default backend kernel]\nIPU: registered at aten/src/ATen/RegisterCompositeExplicitAutogradNonFunctional.cpp:21592 [default backend kernel]\nXPU: registered at aten/src/ATen/RegisterCompositeExplicitAutogradNonFunctional.cpp:21592 [default backend kernel]\nHPU: registered at aten/src/ATen/RegisterCompositeExplicitAutogradNonFunctional.cpp:21592 [default backend kernel]\nVE: registered at aten/src/ATen/RegisterCompositeExplicitAutogradNonFunctional.cpp:21592 [default backend kernel]\nMTIA: registered at aten/src/ATen/RegisterCompositeExplicitAutogradNonFunctional.cpp:21592 [default backend kernel]\nPrivateUse1: registered at aten/src/ATen/RegisterCompositeExplicitAutogradNonFunctional.cpp:21592 [default backend kernel]\nPrivateUse2: registered at aten/src/ATen/RegisterCompositeExplicitAutogradNonFunctional.cpp:21592 [default backend kernel]\nPrivateUse3: registered at aten/src/ATen/RegisterCompositeExplicitAutogradNonFunctional.cpp:21592 [default backend kernel]\nMeta: registered at /dev/null:488 [kernel]\nFPGA: registered at aten/src/ATen/RegisterCompositeExplicitAutogradNonFunctional.cpp:21592 [default backend kernel]\nORT: registered at aten/src/ATen/RegisterCompositeExplicitAutogradNonFunctional.cpp:21592 [default backend kernel]\nVulkan: registered at aten/src/ATen/RegisterCompositeExplicitAutogradNonFunctional.cpp:21592 [default backend kernel]\nMetal: registered at aten/src/ATen/RegisterCompositeExplicitAutogradNonFunctional.cpp:21592 [default backend kernel]\nQuantizedCPU: registered at aten/src/ATen/RegisterQuantizedCPU.cpp:944 [kernel]\nQuantizedCUDA: registered at aten/src/ATen/RegisterCompositeExplicitAutogradNonFunctional.cpp:21592 [default backend kernel]\nQuantizedHIP: registered at aten/src/ATen/RegisterCompositeExplicitAutogradNonFunctional.cpp:21592 [default backend kernel]\nQuantizedMPS: registered at aten/src/ATen/RegisterCompositeExplicitAutogradNonFunctional.cpp:21592 [default backend kernel]\nQuantizedIPU: registered at aten/src/ATen/RegisterCompositeExplicitAutogradNonFunctional.cpp:21592 [default backend kernel]\nQuantizedXPU: registered at aten/src/ATen/RegisterCompositeExplicitAutogradNonFunctional.cpp:21592 [default backend kernel]\nQuantizedHPU: registered at aten/src/ATen/RegisterCompositeExplicitAutogradNonFunctional.cpp:21592 [default backend kernel]\nQuantizedVE: registered at aten/src/ATen/RegisterCompositeExplicitAutogradNonFunctional.cpp:21592 [default backend kernel]\nQuantizedMTIA: registered at aten/src/ATen/RegisterCompositeExplicitAutogradNonFunctional.cpp:21592 [default backend kernel]\nQuantizedPrivateUse1: registered at aten/src/ATen/RegisterCompositeExplicitAutogradNonFunctional.cpp:21592 [default backend kernel]\nQuantizedPrivateUse2: registered at aten/src/ATen/RegisterCompositeExplicitAutogradNonFunctional.cpp:21592 [default backend kernel]\nQuantizedPrivateUse3: registered at aten/src/ATen/RegisterCompositeExplicitAutogradNonFunctional.cpp:21592 [default backend kernel]\nQuantizedMeta: registered at aten/src/ATen/RegisterCompositeExplicitAutogradNonFunctional.cpp:21592 [default backend kernel]\nCustomRNGKeyId: registered at aten/src/ATen/RegisterCompositeExplicitAutogradNonFunctional.cpp:21592 [default backend kernel]\nMkldnnCPU: registered at aten/src/ATen/RegisterCompositeExplicitAutogradNonFunctional.cpp:21592 [default backend kernel]\nSparseCsrCPU: registered at aten/src/ATen/RegisterCompositeExplicitAutogradNonFunctional.cpp:21592 [default backend kernel]\nSparseCsrCUDA: registered at aten/src/ATen/RegisterCompositeExplicitAutogradNonFunctional.cpp:21592 [default backend kernel]\nBackendSelect: fallthrough registered at ../aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:154 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at ../aten/src/ATen/functorch/DynamicLayer.cpp:498 [backend fallback]\nFunctionalize: registered at ../aten/src/ATen/FunctionalizeFallbackKernel.cpp:324 [backend fallback]\nNamed: registered at ../aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at ../aten/src/ATen/ConjugateFallback.cpp:17 [backend fallback]\nNegative: registered at ../aten/src/ATen/native/NegateFallback.cpp:19 [backend fallback]\nZeroTensor: registered at ../aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: fallthrough registered at ../aten/src/ATen/core/VariableFallbackKernel.cpp:86 [backend fallback]\nAutogradOther: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:16254 [autograd kernel]\nAutogradCPU: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:16254 [autograd kernel]\nAutogradCUDA: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:16254 [autograd kernel]\nAutogradHIP: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:16254 [autograd kernel]\nAutogradXLA: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:16254 [autograd kernel]\nAutogradMPS: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:16254 [autograd kernel]\nAutogradIPU: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:16254 [autograd kernel]\nAutogradXPU: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:16254 [autograd kernel]\nAutogradHPU: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:16254 [autograd kernel]\nAutogradVE: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:16254 [autograd kernel]\nAutogradLazy: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:16254 [autograd kernel]\nAutogradMTIA: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:16254 [autograd kernel]\nAutogradPrivateUse1: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:16254 [autograd kernel]\nAutogradPrivateUse2: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:16254 [autograd kernel]\nAutogradPrivateUse3: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:16254 [autograd kernel]\nAutogradMeta: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:16254 [autograd kernel]\nAutogradNestedTensor: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:16254 [autograd kernel]\nTracer: registered at ../torch/csrc/autograd/generated/TraceType_1.cpp:16002 [kernel]\nAutocastCPU: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:378 [backend fallback]\nAutocastCUDA: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:244 [backend fallback]\nFuncTorchBatched: registered at ../aten/src/ATen/functorch/BatchRulesScatterOps.cpp:1242 [kernel]\nBatchedNestedTensor: registered at ../aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:746 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at ../aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at ../aten/src/ATen/LegacyBatchingRegistrations.cpp:1075 [backend fallback]\nVmapMode: fallthrough registered at ../aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at ../aten/src/ATen/functorch/TensorWrapper.cpp:203 [backend fallback]\nPythonTLSSnapshot: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:162 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at ../aten/src/ATen/functorch/DynamicLayer.cpp:494 [backend fallback]\nPreDispatch: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:166 [backend fallback]\nPythonDispatcher: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:158 [backend fallback]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m new_edges \u001b[38;5;241m=\u001b[39m \u001b[43mmake_similar_edges\u001b[49m\u001b[43m(\u001b[49m\u001b[43mauthor_to_conference\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmin_similarity\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.99\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m new_edges\u001b[38;5;241m.\u001b[39mshape\n",
      "Cell \u001b[0;32mIn[11], line 5\u001b[0m, in \u001b[0;36mmake_similar_edges\u001b[0;34m(adjacency_matrix, min_similarity)\u001b[0m\n\u001b[1;32m      2\u001b[0m adjacency_matrix \u001b[38;5;241m=\u001b[39m adjacency_matrix\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m      4\u001b[0m indices \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcombinations(torch\u001b[38;5;241m.\u001b[39marange(adjacency_matrix\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]))\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m----> 5\u001b[0m first_embeddings \u001b[38;5;241m=\u001b[39m \u001b[43madjacency_matrix\u001b[49m\u001b[43m[\u001b[49m\u001b[43mindices\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m      6\u001b[0m second_embeddings \u001b[38;5;241m=\u001b[39m adjacency_matrix[indices[:,\u001b[38;5;241m1\u001b[39m]]\n\u001b[1;32m      7\u001b[0m similarities \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39mcosine_similarity(first_embeddings, second_embeddings, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[0;31mNotImplementedError\u001b[0m: Could not run 'aten::index.Tensor' with arguments from the 'SparseCUDA' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::index.Tensor' is only available for these backends: [CPU, CUDA, HIP, MPS, IPU, XPU, HPU, VE, MTIA, PrivateUse1, PrivateUse2, PrivateUse3, Meta, FPGA, ORT, Vulkan, Metal, QuantizedCPU, QuantizedCUDA, QuantizedHIP, QuantizedMPS, QuantizedIPU, QuantizedXPU, QuantizedHPU, QuantizedVE, QuantizedMTIA, QuantizedPrivateUse1, QuantizedPrivateUse2, QuantizedPrivateUse3, QuantizedMeta, CustomRNGKeyId, MkldnnCPU, SparseCsrCPU, SparseCsrCUDA, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMTIA, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradMeta, AutogradNestedTensor, Tracer, AutocastCPU, AutocastCUDA, FuncTorchBatched, BatchedNestedTensor, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PreDispatch, PythonDispatcher].\n\nUndefined: registered at aten/src/ATen/RegisterCompositeExplicitAutogradNonFunctional.cpp:21592 [default backend kernel]\nCPU: registered at aten/src/ATen/RegisterCPU.cpp:31357 [kernel]\nCUDA: registered at aten/src/ATen/RegisterCUDA.cpp:44411 [kernel]\nHIP: registered at aten/src/ATen/RegisterCompositeExplicitAutogradNonFunctional.cpp:21592 [default backend kernel]\nMPS: registered at aten/src/ATen/RegisterCompositeExplicitAutogradNonFunctional.cpp:21592 [default backend kernel]\nIPU: registered at aten/src/ATen/RegisterCompositeExplicitAutogradNonFunctional.cpp:21592 [default backend kernel]\nXPU: registered at aten/src/ATen/RegisterCompositeExplicitAutogradNonFunctional.cpp:21592 [default backend kernel]\nHPU: registered at aten/src/ATen/RegisterCompositeExplicitAutogradNonFunctional.cpp:21592 [default backend kernel]\nVE: registered at aten/src/ATen/RegisterCompositeExplicitAutogradNonFunctional.cpp:21592 [default backend kernel]\nMTIA: registered at aten/src/ATen/RegisterCompositeExplicitAutogradNonFunctional.cpp:21592 [default backend kernel]\nPrivateUse1: registered at aten/src/ATen/RegisterCompositeExplicitAutogradNonFunctional.cpp:21592 [default backend kernel]\nPrivateUse2: registered at aten/src/ATen/RegisterCompositeExplicitAutogradNonFunctional.cpp:21592 [default backend kernel]\nPrivateUse3: registered at aten/src/ATen/RegisterCompositeExplicitAutogradNonFunctional.cpp:21592 [default backend kernel]\nMeta: registered at /dev/null:488 [kernel]\nFPGA: registered at aten/src/ATen/RegisterCompositeExplicitAutogradNonFunctional.cpp:21592 [default backend kernel]\nORT: registered at aten/src/ATen/RegisterCompositeExplicitAutogradNonFunctional.cpp:21592 [default backend kernel]\nVulkan: registered at aten/src/ATen/RegisterCompositeExplicitAutogradNonFunctional.cpp:21592 [default backend kernel]\nMetal: registered at aten/src/ATen/RegisterCompositeExplicitAutogradNonFunctional.cpp:21592 [default backend kernel]\nQuantizedCPU: registered at aten/src/ATen/RegisterQuantizedCPU.cpp:944 [kernel]\nQuantizedCUDA: registered at aten/src/ATen/RegisterCompositeExplicitAutogradNonFunctional.cpp:21592 [default backend kernel]\nQuantizedHIP: registered at aten/src/ATen/RegisterCompositeExplicitAutogradNonFunctional.cpp:21592 [default backend kernel]\nQuantizedMPS: registered at aten/src/ATen/RegisterCompositeExplicitAutogradNonFunctional.cpp:21592 [default backend kernel]\nQuantizedIPU: registered at aten/src/ATen/RegisterCompositeExplicitAutogradNonFunctional.cpp:21592 [default backend kernel]\nQuantizedXPU: registered at aten/src/ATen/RegisterCompositeExplicitAutogradNonFunctional.cpp:21592 [default backend kernel]\nQuantizedHPU: registered at aten/src/ATen/RegisterCompositeExplicitAutogradNonFunctional.cpp:21592 [default backend kernel]\nQuantizedVE: registered at aten/src/ATen/RegisterCompositeExplicitAutogradNonFunctional.cpp:21592 [default backend kernel]\nQuantizedMTIA: registered at aten/src/ATen/RegisterCompositeExplicitAutogradNonFunctional.cpp:21592 [default backend kernel]\nQuantizedPrivateUse1: registered at aten/src/ATen/RegisterCompositeExplicitAutogradNonFunctional.cpp:21592 [default backend kernel]\nQuantizedPrivateUse2: registered at aten/src/ATen/RegisterCompositeExplicitAutogradNonFunctional.cpp:21592 [default backend kernel]\nQuantizedPrivateUse3: registered at aten/src/ATen/RegisterCompositeExplicitAutogradNonFunctional.cpp:21592 [default backend kernel]\nQuantizedMeta: registered at aten/src/ATen/RegisterCompositeExplicitAutogradNonFunctional.cpp:21592 [default backend kernel]\nCustomRNGKeyId: registered at aten/src/ATen/RegisterCompositeExplicitAutogradNonFunctional.cpp:21592 [default backend kernel]\nMkldnnCPU: registered at aten/src/ATen/RegisterCompositeExplicitAutogradNonFunctional.cpp:21592 [default backend kernel]\nSparseCsrCPU: registered at aten/src/ATen/RegisterCompositeExplicitAutogradNonFunctional.cpp:21592 [default backend kernel]\nSparseCsrCUDA: registered at aten/src/ATen/RegisterCompositeExplicitAutogradNonFunctional.cpp:21592 [default backend kernel]\nBackendSelect: fallthrough registered at ../aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:154 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at ../aten/src/ATen/functorch/DynamicLayer.cpp:498 [backend fallback]\nFunctionalize: registered at ../aten/src/ATen/FunctionalizeFallbackKernel.cpp:324 [backend fallback]\nNamed: registered at ../aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at ../aten/src/ATen/ConjugateFallback.cpp:17 [backend fallback]\nNegative: registered at ../aten/src/ATen/native/NegateFallback.cpp:19 [backend fallback]\nZeroTensor: registered at ../aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: fallthrough registered at ../aten/src/ATen/core/VariableFallbackKernel.cpp:86 [backend fallback]\nAutogradOther: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:16254 [autograd kernel]\nAutogradCPU: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:16254 [autograd kernel]\nAutogradCUDA: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:16254 [autograd kernel]\nAutogradHIP: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:16254 [autograd kernel]\nAutogradXLA: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:16254 [autograd kernel]\nAutogradMPS: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:16254 [autograd kernel]\nAutogradIPU: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:16254 [autograd kernel]\nAutogradXPU: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:16254 [autograd kernel]\nAutogradHPU: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:16254 [autograd kernel]\nAutogradVE: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:16254 [autograd kernel]\nAutogradLazy: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:16254 [autograd kernel]\nAutogradMTIA: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:16254 [autograd kernel]\nAutogradPrivateUse1: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:16254 [autograd kernel]\nAutogradPrivateUse2: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:16254 [autograd kernel]\nAutogradPrivateUse3: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:16254 [autograd kernel]\nAutogradMeta: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:16254 [autograd kernel]\nAutogradNestedTensor: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:16254 [autograd kernel]\nTracer: registered at ../torch/csrc/autograd/generated/TraceType_1.cpp:16002 [kernel]\nAutocastCPU: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:378 [backend fallback]\nAutocastCUDA: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:244 [backend fallback]\nFuncTorchBatched: registered at ../aten/src/ATen/functorch/BatchRulesScatterOps.cpp:1242 [kernel]\nBatchedNestedTensor: registered at ../aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:746 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at ../aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at ../aten/src/ATen/LegacyBatchingRegistrations.cpp:1075 [backend fallback]\nVmapMode: fallthrough registered at ../aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at ../aten/src/ATen/functorch/TensorWrapper.cpp:203 [backend fallback]\nPythonTLSSnapshot: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:162 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at ../aten/src/ATen/functorch/DynamicLayer.cpp:494 [backend fallback]\nPreDispatch: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:166 [backend fallback]\nPythonDispatcher: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:158 [backend fallback]\n"
     ]
    }
   ],
   "source": [
    "new_edges = make_similar_edges(author_to_conference, min_similarity=0.99)\n",
    "new_edges.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['author', 'to', 'author'].edge_index = new_edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HeteroData(\n",
       "  author={\n",
       "    x=[4057, 334],\n",
       "    y=[4057],\n",
       "    train_mask=[4057],\n",
       "    val_mask=[4057],\n",
       "    test_mask=[4057],\n",
       "  },\n",
       "  paper={ x=[14328, 4231] },\n",
       "  term={ x=[7723, 50] },\n",
       "  conference={\n",
       "    num_nodes=20,\n",
       "    x=[20, 1],\n",
       "  },\n",
       "  (author, to, paper)={ edge_index=[2, 9823] },\n",
       "  (paper, to, author)={\n",
       "    edge_index=[2, 9823],\n",
       "    edge_label=[9823],\n",
       "    edge_label_index=[2, 9823],\n",
       "  },\n",
       "  (paper, to, term)={\n",
       "    edge_index=[2, 42905],\n",
       "    edge_label=[42905],\n",
       "    edge_label_index=[2, 42905],\n",
       "  },\n",
       "  (paper, to, conference)={\n",
       "    edge_index=[2, 7164],\n",
       "    edge_label=[7164],\n",
       "    edge_label_index=[2, 7164],\n",
       "  },\n",
       "  (term, to, paper)={ edge_index=[2, 42905] },\n",
       "  (conference, to, paper)={ edge_index=[2, 7164] },\n",
       "  (author, to, author)={ edge_index=[2, 102906] }\n",
       ")"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enhanced: Best Epoch: 11 Train: 0.9750, Val: 0.8250, Test: 0.8293\n"
     ]
    }
   ],
   "source": [
    "graph_learning.set_seed()\n",
    "data_copy = deepcopy(data)\n",
    "data_copy.to(device)\n",
    "model, optimizer = graph_learning.init_parameters(data_copy, device)\n",
    "train_accs, val_accs, test_accs = [],[],[]\n",
    "for epoch in range(1, 100):\n",
    "    loss = graph_learning.train_epoch(data=data_copy, model=model, optimizer=optimizer)\n",
    "    train_acc, val_acc, test_acc = graph_learning.test_epoch(data = data_copy, model=model)\n",
    "    train_accs.append(train_acc)\n",
    "    val_accs.append(val_acc)\n",
    "    test_accs.append(test_acc)\n",
    "best_epoch = max(enumerate(val_accs),key=lambda x: x[1])[0]\n",
    "train_acc, val_acc, test_acc = train_accs[best_epoch], val_accs[best_epoch], test_accs[best_epoch]\n",
    "print(f'Enhanced: Best Epoch: {best_epoch} Train: {train_acc:.4f}, '\n",
    "        f'Val: {val_acc:.4f}, Test: {test_acc:.4f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
