{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HeteroData(\n",
      "  author={\n",
      "    x=[4057, 334],\n",
      "    y=[4057],\n",
      "    train_mask=[4057],\n",
      "    val_mask=[4057],\n",
      "    test_mask=[4057],\n",
      "  },\n",
      "  paper={ x=[14328, 4231] },\n",
      "  term={ x=[7723, 50] },\n",
      "  conference={\n",
      "    num_nodes=20,\n",
      "    x=[20, 1],\n",
      "  },\n",
      "  (author, to, paper)={ edge_index=[2, 19645] },\n",
      "  (paper, to, author)={ edge_index=[2, 19645] },\n",
      "  (paper, to, term)={ edge_index=[2, 85810] },\n",
      "  (paper, to, conference)={ edge_index=[2, 14328] },\n",
      "  (term, to, paper)={ edge_index=[2, 85810] },\n",
      "  (conference, to, paper)={ edge_index=[2, 14328] }\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import os.path as osp\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "import torch\n",
    "from torch_geometric.datasets import DBLP\n",
    "from torch_geometric.utils import negative_sampling\n",
    "from modules.edge_predictor import EdgePredictor\n",
    "\n",
    "import torch_geometric.transforms as T\n",
    "import torch.nn.functional as F\n",
    "import graph_polluters\n",
    "import graph_learning\n",
    "\n",
    "\n",
    "path = './data/dblp'\n",
    "# We initialize conference node features with a single one-vector as feature:\n",
    "dataset = DBLP(path, transform=T.Constant(node_types='conference'))\n",
    "data = dataset[0]\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['author'][''] = torch.zeros_like(data['author'].x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HeteroData(\n",
       "  author={\n",
       "    x=[4057, 334],\n",
       "    y=[4057],\n",
       "    train_mask=[4057],\n",
       "    val_mask=[4057],\n",
       "    test_mask=[4057],\n",
       "    thing=[4057, 334],\n",
       "  },\n",
       "  paper={ x=[14328, 4231] },\n",
       "  term={ x=[7723, 50] },\n",
       "  conference={\n",
       "    num_nodes=20,\n",
       "    x=[20, 1],\n",
       "  },\n",
       "  (author, to, paper)={ edge_index=[2, 19645] },\n",
       "  (paper, to, author)={ edge_index=[2, 19645] },\n",
       "  (paper, to, term)={ edge_index=[2, 85810] },\n",
       "  (paper, to, conference)={ edge_index=[2, 14328] },\n",
       "  (term, to, paper)={ edge_index=[2, 85810] },\n",
       "  (conference, to, paper)={ edge_index=[2, 14328] }\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "edge_predictor = EdgePredictor(data.metadata(), hidden_channels=10, num_layers=2)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "data, edge_predictor = data.to(device), edge_predictor.to(device)\n",
    "\n",
    "edge_types = [(\"paper\", \"to\", \"author\"), (\"paper\", \"to\", \"term\"), (\"paper\", \"to\", \"conference\")]\n",
    "rev_edge_types = [(\"author\", \"to\", \"paper\"), (\"term\", \"to\", \"paper\"), (\"conference\", \"to\", \"paper\")]\n",
    "split_transform = T.RandomLinkSplit(num_val = 0.2, num_test=0, edge_types=edge_types, rev_edge_types=rev_edge_types)\n",
    "\n",
    "#todo add train data\n",
    "train_data, val_data, _ = split_transform(data)\n",
    "\n",
    "with torch.no_grad():  # Initialize lazy modules.\n",
    "    out = edge_predictor(data.x_dict, data.edge_index_dict)\n",
    "\n",
    "optimizer = torch.optim.Adam(edge_predictor.parameters(), lr=0.005, weight_decay=0.001)\n",
    "\n",
    "\n",
    "def train_predictor(train_data):\n",
    "    edge_predictor.train()\n",
    "    optimizer.zero_grad()\n",
    "    embedding = edge_predictor(train_data.x_dict, train_data.edge_index_dict)\n",
    "\n",
    "    similarities, labels = test_random_edges(embedding, train_data, 1000, 1000)\n",
    "\n",
    "    loss = F.binary_cross_entropy(similarities, labels)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return float(loss)\n",
    "\n",
    "def test_random_edges(embedding, data, num_pos, num_neg):\n",
    "    '''\n",
    "    Return the similarity values of positive and negative examples of edges\n",
    "    '''\n",
    "    similarities, labels = [],[]\n",
    "    for edge_type in edge_types:\n",
    "        # Get positive examples\n",
    "        indices = torch.randperm(data.edge_index_dict[edge_type].size()[1])[:num_pos]\n",
    "        pos_examples = data.edge_index_dict[edge_type][:,indices]\n",
    "        pos_labels = torch.ones(num_pos, device=device)\n",
    "    \n",
    "        # Get negative examples\n",
    "        num_nodes = data[edge_type[0]]['x'].size()[0], data[edge_type[2]]['x'].size()[0]\n",
    "        neg_examples = negative_sampling(\n",
    "            data.edge_index_dict[edge_type],\n",
    "            num_nodes=num_nodes,\n",
    "            force_undirected=True,\n",
    "            num_neg_samples=num_neg\n",
    "        )\n",
    "        neg_labels = torch.rand(neg_examples.size()[1], device=device)\n",
    "\n",
    "        # concatanate and shuffle\n",
    "        indices = torch.randperm(num_pos+num_neg)\n",
    "        edges = torch.cat((pos_examples, neg_examples), 1)[:,indices]\n",
    "        labels.append(torch.cat((pos_labels, neg_labels))[indices])\n",
    "        similarities.append(test_edges_similarity(edges, embedding, edge_type[0], edge_type[2]))\n",
    "    similarities, labels = torch.concat(similarities), torch.concat(labels)\n",
    "    return similarities, labels\n",
    "\n",
    "def test_edges_similarity(edges, embedding, node_type1, node_type2):\n",
    "        '''\n",
    "        Find the cosine similarity of the given tensor of edges (from node_type1 to node_type2) \n",
    "        with regard to the given embedding\n",
    "        '''\n",
    "        # get embeddings for the nodes in the edges\n",
    "        x1 = torch.index_select(embedding[node_type1], 0, edges[0])\n",
    "        x2 = torch.index_select(embedding[node_type2], 0, edges[1])\n",
    "        # calculate cosine similarity\n",
    "        return torch.cosine_similarity(x1, x2, dim=1)/2 + 0.5\n",
    "\n",
    "@torch.no_grad()\n",
    "def test_predictor(test_data):\n",
    "    edge_predictor.eval()\n",
    "    embedding = edge_predictor(test_data.x_dict, test_data.edge_index_dict)\n",
    "    similarities, labels = test_random_edges(embedding, test_data, 1000, 1000)\n",
    "    loss = F.binary_cross_entropy(similarities, labels)\n",
    "    return loss\n",
    "\n",
    "@torch.no_grad()\n",
    "def add_potential_edges(model, data, similarity_theshold = 0.9, number_to_test=100000):\n",
    "    embedding = model(train_data.x_dict, train_data.edge_index_dict)\n",
    "    for edge_type in edge_types:\n",
    "        # Get negative examples\n",
    "        num_nodes = data[edge_type[0]]['x'].size()[0], data[edge_type[2]]['x'].size()[0]\n",
    "        edges = negative_sampling(\n",
    "            data.edge_index_dict[edge_type],\n",
    "            num_nodes=num_nodes,\n",
    "            force_undirected=True,\n",
    "            num_neg_samples=number_to_test\n",
    "        )\n",
    "        similarities = test_edges_similarity(edges, embedding, edge_type[0], edge_type[2])\n",
    "        new_edges = edges[:,similarities > similarity_theshold]\n",
    "        data[edge_type].edge_index = torch.cat((data[edge_type].edge_index, new_edges), 1)\n",
    "        data[edge_type[::-1]].edge_index = torch.cat((data[edge_type[::-1]].edge_index, new_edges[[1,0],:]), 1)\n",
    "        print(f'Added {new_edges.size()[1]} edges')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'edge_index': tensor([[ 1039,  1005,  1545,  ...,   274,  3885,  1151],\n",
       "        [ 1447,  6888,  9418,  ...,   123, 14243, 11934]], device='cuda:0')}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[('author', 'to', 'paper')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:11<00:00, 17.67it/s]\n"
     ]
    }
   ],
   "source": [
    "#todo save best model\n",
    "for epoch in tqdm(range(1, 201)):\n",
    "    train_loss = train_predictor(train_data)\n",
    "    test_loss = test_predictor(val_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline, Train: 1.0000, Val: 0.8025, Test: 0.8090\n"
     ]
    }
   ],
   "source": [
    "graph_learning.set_seed()\n",
    "dataset_copy = dataset.copy()\n",
    "data_copy = dataset_copy[0]\n",
    "data_copy.to(device)\n",
    "model, optimizer = graph_learning.init_parameters(data_copy)\n",
    "train_accs, val_accs, test_accs = [],[],[]\n",
    "for epoch in range(1, 100):\n",
    "    loss = graph_learning.train_epoch(data=data_copy, model=model, optimizer=optimizer)\n",
    "    train_acc, val_acc, test_acc = graph_learning.test_epoch(data = data_copy, model=model)\n",
    "    train_accs.append(train_acc)\n",
    "    val_accs.append(val_acc)\n",
    "    test_accs.append(test_acc)\n",
    "best_epoch = max(enumerate(val_accs),key=lambda x: x[1])[0]\n",
    "train_acc, val_acc, test_acc = train_accs[best_epoch], val_accs[best_epoch], test_accs[best_epoch]\n",
    "print(f'Baseline, Train: {train_acc:.4f}, '\n",
    "        f'Val: {val_acc:.4f}, Test: {test_acc:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 50% Sparse Edges\n",
    "todo more percentages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline, Train: 1.0000, Val: 0.7400, Test: 0.7691\n"
     ]
    }
   ],
   "source": [
    "graph_learning.set_seed()\n",
    "dataset_copy = dataset.copy()\n",
    "data_copy = dataset_copy[0]\n",
    "data_copy.to(device)\n",
    "data_copy = graph_polluters.remove_edges(data_copy, 0.5)\n",
    "model, optimizer = graph_learning.init_parameters(data_copy)\n",
    "train_accs, val_accs, test_accs = [],[],[]\n",
    "for epoch in range(1, 100):\n",
    "    loss = graph_learning.train_epoch(data=data_copy, model=model, optimizer=optimizer)\n",
    "    train_acc, val_acc, test_acc = graph_learning.test_epoch(data = data_copy, model=model)\n",
    "    train_accs.append(train_acc)\n",
    "    val_accs.append(val_acc)\n",
    "    test_accs.append(test_acc)\n",
    "best_epoch = max(enumerate(val_accs),key=lambda x: x[1])[0]\n",
    "train_acc, val_acc, test_acc = train_accs[best_epoch], val_accs[best_epoch], test_accs[best_epoch]\n",
    "print(f'Baseline, Train: {train_acc:.4f}, '\n",
    "        f'Val: {val_acc:.4f}, Test: {test_acc:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 50% Sparse Edges + Pseudoedges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added 4176 edges\n",
      "Added 2622 edges\n",
      "Added 5445 edges\n",
      "Baseline, Train: 1.0000, Val: 0.7300, Test: 0.7749\n"
     ]
    }
   ],
   "source": [
    "graph_learning.set_seed()\n",
    "\n",
    "#copy the dataset\n",
    "dataset_copy = dataset.copy()\n",
    "data_copy = dataset_copy[0]\n",
    "data_copy.to(device)\n",
    "\n",
    "# remove edges\n",
    "data_copy = graph_polluters.remove_edges(data_copy, 0.5)\n",
    "\n",
    "# add potential edges\n",
    "add_potential_edges(edge_predictor, data_copy, 0.9)\n",
    "\n",
    "model, optimizer = graph_learning.init_parameters(data_copy)\n",
    "train_accs, val_accs, test_accs = [],[],[]\n",
    "for epoch in range(1, 100):\n",
    "    loss = graph_learning.train_epoch(data=data_copy, model=model, optimizer=optimizer)\n",
    "    train_acc, val_acc, test_acc = graph_learning.test_epoch(data = data_copy, model=model)\n",
    "    train_accs.append(train_acc)\n",
    "    val_accs.append(val_acc)\n",
    "    test_accs.append(test_acc)\n",
    "best_epoch = max(enumerate(val_accs),key=lambda x: x[1])[0]\n",
    "train_acc, val_acc, test_acc = train_accs[best_epoch], val_accs[best_epoch], test_accs[best_epoch]\n",
    "print(f'Baseline, Train: {train_acc:.4f}, '\n",
    "        f'Val: {val_acc:.4f}, Test: {test_acc:.4f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
