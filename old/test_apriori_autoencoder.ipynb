{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (graph_polluters.py, line 36)",
     "output_type": "error",
     "traceback": [
      "Traceback \u001b[0;36m(most recent call last)\u001b[0m:\n",
      "\u001b[0m  File \u001b[1;32m/home/iailab42/kanea0/my-env/lib/python3.8/site-packages/IPython/core/interactiveshell.py:3508\u001b[0m in \u001b[1;35mrun_code\u001b[0m\n    exec(code_obj, self.user_global_ns, self.user_ns)\u001b[0m\n",
      "\u001b[0;36m  Cell \u001b[0;32mIn[2], line 6\u001b[0;36m\n\u001b[0;31m    import graph_polluters\u001b[0;36m\n",
      "\u001b[0;36m  File \u001b[0;32m/home/iailab42/kanea0/graph-learning/graph_polluters.py:36\u001b[0;36m\u001b[0m\n\u001b[0;31m    split_transform = T.RandomLinkSplit(num_val = probability, num_test=0, add_negative_train_samples=False edge_types=edge_types, rev_edge_types=rev_edge_types)\u001b[0m\n\u001b[0m                                                                                                            ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch_geometric.datasets import DBLP\n",
    "\n",
    "import torch_geometric.transforms as T\n",
    "import torch.nn.functional as F\n",
    "import graph_polluters\n",
    "import graph_learning\n",
    "import apriori\n",
    "import autoencoder\n",
    "\n",
    "\n",
    "\n",
    "path = './data/dblp'\n",
    "# We initialize conference node features with a single one-vector as feature:\n",
    "dataset = DBLP(path, transform=T.Constant(node_types='conference'))\n",
    "data = dataset[0]\n",
    "print(data)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 7., 22.,  7.,  ...,  0.,  0.,  4.])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.sum(data['author'].x, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline, Train: 1.0000, Val: 0.8025, Test: 0.8090\n"
     ]
    }
   ],
   "source": [
    "graph_learning.set_seed()\n",
    "dataset_copy = dataset.copy()\n",
    "data_copy = dataset_copy[0]\n",
    "data_copy.to(device)\n",
    "model, optimizer = graph_learning.init_parameters(data_copy, device)\n",
    "train_accs, val_accs, test_accs = [],[],[]\n",
    "for epoch in range(1, 100):\n",
    "    loss = graph_learning.train(data=data_copy, model=model, optimizer=optimizer)\n",
    "    train_acc, val_acc, test_acc = graph_learning.test(data = data_copy, model=model)\n",
    "    train_accs.append(train_acc)\n",
    "    val_accs.append(val_acc)\n",
    "    test_accs.append(test_acc)\n",
    "best_epoch = max(enumerate(val_accs),key=lambda x: x[1])[0]\n",
    "train_acc, val_acc, test_acc = train_accs[best_epoch], val_accs[best_epoch], test_accs[best_epoch]\n",
    "print(f'Baseline, Train: {train_acc:.4f}, '\n",
    "        f'Val: {val_acc:.4f}, Test: {test_acc:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 50% features removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline, Train: 1.0000, Val: 0.7150, Test: 0.7283\n"
     ]
    }
   ],
   "source": [
    "graph_learning.set_seed()\n",
    "\n",
    "#copy the dataset\n",
    "dataset_copy = dataset.copy()\n",
    "data_copy = dataset_copy[0]\n",
    "data_copy.to(device)\n",
    "data_copy.cpu()\n",
    "\n",
    "# remove features\n",
    "data_copy = graph_polluters.remove_features(data_copy, 0.5)\n",
    "\n",
    "model, optimizer = graph_learning.init_parameters(data_copy, device)\n",
    "train_accs, val_accs, test_accs = [],[],[]\n",
    "for epoch in range(1, 100):\n",
    "    loss = graph_learning.train(data=data_copy, model=model, optimizer=optimizer)\n",
    "    train_acc, val_acc, test_acc = graph_learning.test(data = data_copy, model=model)\n",
    "    train_accs.append(train_acc)\n",
    "    val_accs.append(val_acc)\n",
    "    test_accs.append(test_acc)\n",
    "best_epoch = max(enumerate(val_accs),key=lambda x: x[1])[0]\n",
    "train_acc, val_acc, test_acc = train_accs[best_epoch], val_accs[best_epoch], test_accs[best_epoch]\n",
    "print(f'Baseline, Train: {train_acc:.4f}, '\n",
    "        f'Val: {val_acc:.4f}, Test: {test_acc:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 50% features removed + apriori added"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline, Train: 1.0000, Val: 0.7900, Test: 0.7937\n"
     ]
    }
   ],
   "source": [
    "graph_learning.set_seed()\n",
    "\n",
    "#copy the dataset\n",
    "dataset_copy = dataset.copy()\n",
    "data_copy = dataset_copy[0]\n",
    "#data_copy.to(device)\n",
    "\n",
    "data_copy = graph_polluters.remove_features(data_copy, 0.5)\n",
    "\n",
    "item_sets = apriori.find_frequent_sets(data_copy, 'author', min_support = 0.01)\n",
    "apriori.conflate_item_sets(data_copy, 'author', item_sets)\n",
    "# remove features\n",
    "# add new features\n",
    "\n",
    "model, optimizer = graph_learning.init_parameters(data_copy,device)\n",
    "train_accs, val_accs, test_accs = [],[],[]\n",
    "for epoch in range(1, 100):\n",
    "    loss = graph_learning.train(data=data_copy, model=model, optimizer=optimizer)\n",
    "    train_acc, val_acc, test_acc = graph_learning.test(data = data_copy, model=model)\n",
    "    train_accs.append(train_acc)\n",
    "    val_accs.append(val_acc)\n",
    "    test_accs.append(test_acc)\n",
    "best_epoch = max(enumerate(val_accs),key=lambda x: x[1])[0]\n",
    "train_acc, val_acc, test_acc = train_accs[best_epoch], val_accs[best_epoch], test_accs[best_epoch]\n",
    "print(f'Baseline, Train: {train_acc:.4f}, '\n",
    "        f'Val: {val_acc:.4f}, Test: {test_acc:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 50% features removed + autoencoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline, Train: 1.0000, Val: 0.7800, Test: 0.7915\n"
     ]
    }
   ],
   "source": [
    "graph_learning.set_seed()\n",
    "\n",
    "#copy the dataset\n",
    "dataset_copy = dataset.copy()\n",
    "data_copy = dataset_copy[0]\n",
    "data_copy.to(device)\n",
    "\n",
    "# remove features\n",
    "data_copy = graph_polluters.remove_features(data_copy, 0.5, ['author'])\n",
    "\n",
    "# add new features\n",
    "\n",
    "\n",
    "model, optimizer = graph_learning.init_parameters(data_copy, device)\n",
    "train_accs, val_accs, test_accs = [],[],[]\n",
    "for epoch in range(1, 100):\n",
    "    loss = graph_learning.train(data=data_copy, model=model, optimizer=optimizer)\n",
    "    train_acc, val_acc, test_acc = graph_learning.test(data = data_copy, model=model)\n",
    "    train_accs.append(train_acc)\n",
    "    val_accs.append(val_acc)\n",
    "    test_accs.append(test_acc)\n",
    "best_epoch = max(enumerate(val_accs),key=lambda x: x[1])[0]\n",
    "train_acc, val_acc, test_acc = train_accs[best_epoch], val_accs[best_epoch], test_accs[best_epoch]\n",
    "print(f'Baseline, Train: {train_acc:.4f}, '\n",
    "        f'Val: {val_acc:.4f}, Test: {test_acc:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m data_copy \u001b[38;5;241m=\u001b[39m dataset_copy[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m#data_copy.to(device)\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m item_sets \u001b[38;5;241m=\u001b[39m \u001b[43mapriori\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind_frequent_sets\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_copy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mauthor\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmin_support\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.02\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(item_sets)\n\u001b[1;32m     10\u001b[0m apriori\u001b[38;5;241m.\u001b[39mconflate_item_sets(data_copy, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mauthor\u001b[39m\u001b[38;5;124m'\u001b[39m, item_sets)\n",
      "File \u001b[0;32m/home/iailab42/kanea0/graph-learning/apriori.py:8\u001b[0m, in \u001b[0;36mfind_frequent_sets\u001b[0;34m(data, node_type, min_support, min_confidence, min_lift, max_length)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfind_frequent_sets\u001b[39m(data, node_type, min_support\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m, min_confidence \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m, min_lift\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, max_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;66;03m# Might be able to make this faster for huge datasets\u001b[39;00m\n\u001b[1;32m      7\u001b[0m     collists \u001b[38;5;241m=\u001b[39m [torch\u001b[38;5;241m.\u001b[39mnonzero(t)\u001b[38;5;241m.\u001b[39msqueeze(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mtolist() \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m data[node_type][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mx\u001b[39m\u001b[38;5;124m'\u001b[39m]]\n\u001b[0;32m----> 8\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mapriori\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcollists\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmin_support\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmin_support\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmin_confidence\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmin_confidence\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmin_lift\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmin_lift\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_length\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m[\u001b[38;5;28mlist\u001b[39m(result\u001b[38;5;241m.\u001b[39mitems) \u001b[38;5;28;01mfor\u001b[39;00m result \u001b[38;5;129;01min\u001b[39;00m results \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(result\u001b[38;5;241m.\u001b[39mitems) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m]\n",
      "File \u001b[0;32m/home/iailab42/kanea0/my-env/lib/python3.8/site-packages/apyori.py:287\u001b[0m, in \u001b[0;36mapriori\u001b[0;34m(transactions, **kwargs)\u001b[0m\n\u001b[1;32m    285\u001b[0m \u001b[38;5;66;03m# Calculate ordered stats.\u001b[39;00m\n\u001b[1;32m    286\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m support_record \u001b[38;5;129;01min\u001b[39;00m support_records:\n\u001b[0;32m--> 287\u001b[0m     ordered_statistics \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    288\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_filter_ordered_statistics\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    289\u001b[0m \u001b[43m            \u001b[49m\u001b[43m_gen_ordered_statistics\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtransaction_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msupport_record\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    290\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmin_confidence\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmin_confidence\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    291\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmin_lift\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmin_lift\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    292\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    293\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    294\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m ordered_statistics:\n\u001b[1;32m    295\u001b[0m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n",
      "File \u001b[0;32m/home/iailab42/kanea0/my-env/lib/python3.8/site-packages/apyori.py:237\u001b[0m, in \u001b[0;36mfilter_ordered_statistics\u001b[0;34m(ordered_statistics, **kwargs)\u001b[0m\n\u001b[1;32m    234\u001b[0m min_confidence \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmin_confidence\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;241m0.0\u001b[39m)\n\u001b[1;32m    235\u001b[0m min_lift \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmin_lift\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;241m0.0\u001b[39m)\n\u001b[0;32m--> 237\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m ordered_statistic \u001b[38;5;129;01min\u001b[39;00m ordered_statistics:\n\u001b[1;32m    238\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ordered_statistic\u001b[38;5;241m.\u001b[39mconfidence \u001b[38;5;241m<\u001b[39m min_confidence:\n\u001b[1;32m    239\u001b[0m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n",
      "File \u001b[0;32m/home/iailab42/kanea0/my-env/lib/python3.8/site-packages/apyori.py:218\u001b[0m, in \u001b[0;36mgen_ordered_statistics\u001b[0;34m(transaction_manager, record)\u001b[0m\n\u001b[1;32m    215\u001b[0m items_add \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mfrozenset\u001b[39m(items\u001b[38;5;241m.\u001b[39mdifference(items_base))\n\u001b[1;32m    216\u001b[0m confidence \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    217\u001b[0m     record\u001b[38;5;241m.\u001b[39msupport \u001b[38;5;241m/\u001b[39m transaction_manager\u001b[38;5;241m.\u001b[39mcalc_support(items_base))\n\u001b[0;32m--> 218\u001b[0m lift \u001b[38;5;241m=\u001b[39m confidence \u001b[38;5;241m/\u001b[39m \u001b[43mtransaction_manager\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcalc_support\u001b[49m\u001b[43m(\u001b[49m\u001b[43mitems_add\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    219\u001b[0m \u001b[38;5;28;01myield\u001b[39;00m OrderedStatistic(\n\u001b[1;32m    220\u001b[0m     \u001b[38;5;28mfrozenset\u001b[39m(items_base), \u001b[38;5;28mfrozenset\u001b[39m(items_add), confidence, lift)\n",
      "File \u001b[0;32m/home/iailab42/kanea0/my-env/lib/python3.8/site-packages/apyori.py:88\u001b[0m, in \u001b[0;36mTransactionManager.calc_support\u001b[0;34m(self, items)\u001b[0m\n\u001b[1;32m     85\u001b[0m         sum_indexes \u001b[38;5;241m=\u001b[39m indexes\n\u001b[1;32m     86\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     87\u001b[0m         \u001b[38;5;66;03m# Calculate the intersection on not the first time.\u001b[39;00m\n\u001b[0;32m---> 88\u001b[0m         sum_indexes \u001b[38;5;241m=\u001b[39m \u001b[43msum_indexes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mintersection\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindexes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     90\u001b[0m \u001b[38;5;66;03m# Calculate and return the support.\u001b[39;00m\n\u001b[1;32m     91\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mfloat\u001b[39m(\u001b[38;5;28mlen\u001b[39m(sum_indexes)) \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__num_transaction\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "graph_learning.set_seed()\n",
    "\n",
    "#copy the dataset\n",
    "dataset_copy = dataset.copy()\n",
    "data_copy = dataset_copy[0]\n",
    "#data_copy.to(device)\n",
    "\n",
    "item_sets = apriori.find_frequent_sets(data_copy, 'author', min_support = 0.02)\n",
    "print(item_sets)\n",
    "apriori.conflate_item_sets(data_copy, 'author', item_sets)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
