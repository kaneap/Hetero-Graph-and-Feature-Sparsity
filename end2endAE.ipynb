{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import torch_geometric.transforms as T\n",
    "from torch_geometric.datasets import DBLP\n",
    "import torch.nn.functional as F\n",
    "from graph_polluters import remove_features\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "from tqdm import trange\n",
    "from heteroGNN import HeteroGNN\n",
    "from graph_learning import set_seed\n",
    "from copy import deepcopy\n",
    "\n",
    "dataset = DBLP('./data/dblp', transform=T.Constant(node_types='conference'))\n",
    "data = dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HeteroData(\n",
       "  author={\n",
       "    x=[4057, 334],\n",
       "    y=[4057],\n",
       "    train_mask=[4057],\n",
       "    val_mask=[4057],\n",
       "    test_mask=[4057],\n",
       "  },\n",
       "  paper={ x=[14328, 4231] },\n",
       "  term={ x=[7723, 50] },\n",
       "  conference={\n",
       "    num_nodes=20,\n",
       "    x=[20, 1],\n",
       "  },\n",
       "  (author, to, paper)={ edge_index=[2, 19645] },\n",
       "  (paper, to, author)={ edge_index=[2, 19645] },\n",
       "  (paper, to, term)={ edge_index=[2, 85810] },\n",
       "  (paper, to, conference)={ edge_index=[2, 14328] },\n",
       "  (term, to, paper)={ edge_index=[2, 85810] },\n",
       "  (conference, to, paper)={ edge_index=[2, 14328] }\n",
       ")"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AE(torch.nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dims):\n",
    "        super().__init__()\n",
    "        self.encoder = torch.nn.ModuleList()\n",
    "        self.decoder = torch.nn.ModuleList()\n",
    "\n",
    "        # Encoder\n",
    "        for i in range(len(hidden_dims)):\n",
    "            if i == 0:\n",
    "                self.encoder.append(torch.nn.Linear(input_dim, hidden_dims[i]))\n",
    "            else:\n",
    "                self.encoder.append(torch.nn.Linear(hidden_dims[i-1], hidden_dims[i]))\n",
    "            self.encoder.append(torch.nn.ReLU())\n",
    "        \n",
    "        # Decoder\n",
    "        for i in reversed(range(len(hidden_dims))):\n",
    "            if i == 0:\n",
    "                self.encoder.append(torch.nn.Linear(hidden_dims[i], input_dim))\n",
    "                self.encoder.append(torch.nn.Sigmoid())\n",
    "            else:\n",
    "                self.encoder.append(torch.nn.Linear(hidden_dims[i], hidden_dims[i-1]))\n",
    "                self.encoder.append(torch.nn.ReLU())\n",
    "\n",
    "        self.encoder = torch.nn.Sequential(*self.encoder)\n",
    "        self.decoder = torch.nn.Sequential(*self.decoder)\n",
    "            \n",
    " \n",
    "    def forward(self, x):\n",
    "        encoded = self.encoder(x)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return decoded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pretrain Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_ae(node_type, ae_hidden_dims, epochs=30):\n",
    "    sparse_threshold = 10\n",
    "    not_sparse = torch.sum(data[node_type].x, 1).to(torch.int) > sparse_threshold\n",
    "\n",
    "    base_data = data[node_type].x[not_sparse]\n",
    "    half_data = torch.where(torch.rand_like(base_data) < 0.5, torch.zeros_like(base_data), base_data)\n",
    "\n",
    "    loader = DataLoader(\n",
    "        TensorDataset(base_data, half_data), \n",
    "        batch_size=64, shuffle=True, pin_memory=True)\n",
    "\n",
    "    ae = AE(base_data.shape[-1], ae_hidden_dims)\n",
    "    loss_function = torch.nn.MSELoss()\n",
    "    optimizer = torch.optim.Adam(ae.parameters(),\n",
    "                                lr = 1e-1,\n",
    "                                weight_decay = 1e-8)\n",
    "\n",
    "    outputs = []\n",
    "\n",
    "    average_losses, average_accuracies = [], []\n",
    "    for epoch in trange(epochs):\n",
    "        epoch_losses, epoch_accuracies = [], []\n",
    "        for base, half in loader:\n",
    "            reconstructed = ae(base)\n",
    "            maxed = torch.max(half, reconstructed)\n",
    "            loss = loss_function(maxed, base)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            accuracy = (torch.round(reconstructed) == base).float().mean()\n",
    "            epoch_accuracies.append(accuracy) \n",
    "            # Storing the losses in a list for plotting\n",
    "            epoch_losses.append(loss.item())\n",
    "\n",
    "        average_losses.append(np.mean(epoch_losses))\n",
    "        average_accuracies.append(np.mean(epoch_accuracies))\n",
    "        outputs.append((epochs, base, reconstructed))\n",
    "    return ae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [00:02<00:00, 12.05it/s]\n",
      "100%|██████████| 30/30 [00:04<00:00,  6.25it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "author_ae_dims = [128, 64, 36, 18]\n",
    "paper_ae_dims = [512,128,64,32]\n",
    "author_ae = train_ae('author', ae_hidden_dims = author_ae_dims)\n",
    "paper_ae = train_ae('paper', ae_hidden_dims = paper_ae_dims)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AE GNN Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "def train(data, model, optimizer):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    out, filtered = model(data.x_dict, data.edge_index_dict)\n",
    "    mask = data['author'].train_mask\n",
    "    loss = F.cross_entropy(out[mask], data['author'].y[mask])\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return float(loss)\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def test(data, model):\n",
    "    model.eval()\n",
    "    pred, filtered = model(data.x_dict, data.edge_index_dict)\n",
    "    pred = pred.argmax(dim=-1)\n",
    "\n",
    "    accs = []\n",
    "    for split in ['train_mask', 'val_mask', 'test_mask']:\n",
    "        mask = data['author'][split]\n",
    "        acc = (pred[mask] == data['author'].y[mask]).sum() / mask.sum()\n",
    "        accs.append(float(acc))\n",
    "    return accs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AEGNN(torch.nn.Module):\n",
    "    def __init__(self, ae, gnn):\n",
    "        super().__init__()\n",
    "        self.autoencoders = ae\n",
    "        self.gnn = gnn\n",
    " \n",
    "    def forward(self, x_dict, edge_index_dict):\n",
    "        for node_type, autoencoder in self.autoencoders.items():\n",
    "            filtered = autoencoder(x_dict[node_type])\n",
    "            x_dict[node_type] = filtered\n",
    "        return self.gnn(x_dict, edge_index_dict), filtered"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encode-Decode-GNN Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for AE:\n\tsize mismatch for encoder.0.weight: copying a param with shape torch.Size([512, 334]) from checkpoint, the shape in current model is torch.Size([128, 334]).\n\tsize mismatch for encoder.0.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for encoder.2.weight: copying a param with shape torch.Size([128, 512]) from checkpoint, the shape in current model is torch.Size([64, 128]).\n\tsize mismatch for encoder.2.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for encoder.4.weight: copying a param with shape torch.Size([64, 128]) from checkpoint, the shape in current model is torch.Size([36, 64]).\n\tsize mismatch for encoder.4.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([36]).\n\tsize mismatch for encoder.6.weight: copying a param with shape torch.Size([32, 64]) from checkpoint, the shape in current model is torch.Size([18, 36]).\n\tsize mismatch for encoder.6.bias: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([18]).\n\tsize mismatch for encoder.8.weight: copying a param with shape torch.Size([64, 32]) from checkpoint, the shape in current model is torch.Size([36, 18]).\n\tsize mismatch for encoder.8.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([36]).\n\tsize mismatch for encoder.10.weight: copying a param with shape torch.Size([128, 64]) from checkpoint, the shape in current model is torch.Size([64, 36]).\n\tsize mismatch for encoder.10.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for encoder.12.weight: copying a param with shape torch.Size([512, 128]) from checkpoint, the shape in current model is torch.Size([128, 64]).\n\tsize mismatch for encoder.12.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for encoder.14.weight: copying a param with shape torch.Size([334, 512]) from checkpoint, the shape in current model is torch.Size([334, 128]).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[32], line 11\u001b[0m\n\u001b[1;32m      9\u001b[0m autoencoders \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m()\n\u001b[1;32m     10\u001b[0m autoencoders[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mauthor\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m AE(data_copy[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mauthor\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mx\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m], author_ae_dims)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m---> 11\u001b[0m \u001b[43mautoencoders\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mauthor\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mauthor_ae\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstate_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m autoencoders[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpaper\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m AE(data_copy[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpaper\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mx\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m], paper_ae_dims)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     13\u001b[0m autoencoders[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpaper\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mload_state_dict(paper_ae\u001b[38;5;241m.\u001b[39mstate_dict())\n",
      "File \u001b[0;32m/home/iailab42/kanea0/my-env/lib/python3.12/site-packages/torch/nn/modules/module.py:2153\u001b[0m, in \u001b[0;36mModule.load_state_dict\u001b[0;34m(self, state_dict, strict, assign)\u001b[0m\n\u001b[1;32m   2148\u001b[0m         error_msgs\u001b[38;5;241m.\u001b[39minsert(\n\u001b[1;32m   2149\u001b[0m             \u001b[38;5;241m0\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMissing key(s) in state_dict: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   2150\u001b[0m                 \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m missing_keys)))\n\u001b[1;32m   2152\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(error_msgs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m-> 2153\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mError(s) in loading state_dict for \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   2154\u001b[0m                        \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(error_msgs)))\n\u001b[1;32m   2155\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for AE:\n\tsize mismatch for encoder.0.weight: copying a param with shape torch.Size([512, 334]) from checkpoint, the shape in current model is torch.Size([128, 334]).\n\tsize mismatch for encoder.0.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for encoder.2.weight: copying a param with shape torch.Size([128, 512]) from checkpoint, the shape in current model is torch.Size([64, 128]).\n\tsize mismatch for encoder.2.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for encoder.4.weight: copying a param with shape torch.Size([64, 128]) from checkpoint, the shape in current model is torch.Size([36, 64]).\n\tsize mismatch for encoder.4.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([36]).\n\tsize mismatch for encoder.6.weight: copying a param with shape torch.Size([32, 64]) from checkpoint, the shape in current model is torch.Size([18, 36]).\n\tsize mismatch for encoder.6.bias: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([18]).\n\tsize mismatch for encoder.8.weight: copying a param with shape torch.Size([64, 32]) from checkpoint, the shape in current model is torch.Size([36, 18]).\n\tsize mismatch for encoder.8.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([36]).\n\tsize mismatch for encoder.10.weight: copying a param with shape torch.Size([128, 64]) from checkpoint, the shape in current model is torch.Size([64, 36]).\n\tsize mismatch for encoder.10.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for encoder.12.weight: copying a param with shape torch.Size([512, 128]) from checkpoint, the shape in current model is torch.Size([128, 64]).\n\tsize mismatch for encoder.12.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for encoder.14.weight: copying a param with shape torch.Size([334, 512]) from checkpoint, the shape in current model is torch.Size([334, 128])."
     ]
    }
   ],
   "source": [
    "set_seed()\n",
    "dataset_copy = dataset.copy()\n",
    "data_copy = dataset_copy[0]\n",
    "data_copy = remove_features(data_copy, 0.5)\n",
    "\n",
    "gnn = HeteroGNN(data_copy.metadata(), hidden_channels=10, out_channels=4, num_layers=2)\n",
    "gnn = gnn.to(device)\n",
    "\n",
    "autoencoders = dict()\n",
    "autoencoders['author'] = AE(data_copy['author'].x.shape[-1], author_ae_dims).to(device)\n",
    "autoencoders['author'].load_state_dict(author_ae.state_dict())\n",
    "autoencoders['paper'] = AE(data_copy['paper'].x.shape[-1], paper_ae_dims).to(device)\n",
    "autoencoders['paper'].load_state_dict(paper_ae.state_dict())\n",
    "\n",
    "model = AEGNN(autoencoders, gnn)\n",
    "data_copy, model = data_copy.to(device), model.to(device)\n",
    "\n",
    "\n",
    "\n",
    "with torch.no_grad():  # Initialize lazy modules.\n",
    "    out = model(data_copy.x_dict, data_copy.edge_index_dict)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.005, weight_decay=0.001)\n",
    "\n",
    "train_accs, val_accs, test_accs = [],[],[]\n",
    "for epoch in range(1, 100):\n",
    "    loss = train(data=data_copy, model=model, optimizer=optimizer)\n",
    "    train_acc, val_acc, test_acc = test(data = data_copy, model=model)\n",
    "    train_accs.append(train_acc)\n",
    "    val_accs.append(val_acc)\n",
    "    test_accs.append(test_acc)\n",
    "best_epoch = max(enumerate(val_accs),key=lambda x: x[1])[0]\n",
    "train_acc, val_acc, test_acc = train_accs[best_epoch], val_accs[best_epoch], test_accs[best_epoch]\n",
    "print(f'End 2 End, Train: {train_acc:.4f}, '\n",
    "        f'Val: {val_acc:.4f}, Test: {test_acc:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
